import streamlit as st
import google.generativeai as genai
from docx import Document
from streamlit_mermaid import st_mermaid
from audio_recorder_streamlit import audio_recorder
import tempfile
import os
import time

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="NotebookLM Ultimate", page_icon="üíé", layout="wide")
st.markdown("""<style>.stButton>button {width: 100%; border-radius: 8px; height: 3em; font-weight: bold;}</style>""", unsafe_allow_html=True)

# --- QU·∫¢N L√ù TR·∫†NG TH√ÅI ---
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "gemini_files" not in st.session_state: st.session_state.gemini_files = [] 
if "analysis_result" not in st.session_state: st.session_state.analysis_result = ""

# --- H√ÄM H·ªñ TR·ª¢ ---
def configure_genai():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return True
    except:
        st.error("üö® Ch∆∞a nh·∫≠p API Key trong Secrets!")
        return False

def get_valid_models():
    """H√†m qu√©t danh s√°ch model th·ª±c t·∫ø t·ª´ Google"""
    try:
        models = genai.list_models()
        valid_list = []
        for m in models:
            if 'generateContent' in m.supported_generation_methods:
                valid_list.append(m.name)
        return valid_list
    except:
        return []

def upload_to_gemini(path, mime_type="audio/mp3"):
    file = genai.upload_file(path, mime_type=mime_type)
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    return file

def create_docx(content):
    doc = Document()
    doc.add_heading('NOTEBOOKLM ULTIMATE REPORT', 0)
    for line in content.split('\n'):
        if line.startswith('# '): doc.add_heading(line.replace('# ', ''), level=1)
        elif line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=2)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=3)
        else: doc.add_paragraph(line)
    return doc

# --- MAIN APP ---
def main():
    st.title("üíé NotebookLM Ultimate (Smart Mode)")
    if not configure_genai(): return

    with st.sidebar:
        st.header("üß† Model Engine")
        
        # 1. CH·ªåN MODEL
        model_version = st.selectbox(
            "Ch·ªçn Model:", 
            ("gemini-3.0-flash", "gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash")
        )

        # 2. CH·ªåN CH·∫æ ƒê·ªò X·ª¨ L√ù L·ªñI (T√çNH NƒÇNG M·ªöI)
        fallback_mode = st.radio(
            "Khi Model g·∫∑p l·ªói (404/Overload):",
            ("‚ö° T·ª± ƒë·ªông h·∫° c·∫•p (Auto Fallback)", "üõë D·ª´ng l·∫°i & B√°o c√°o (Manual)")
        )
        
        st.divider()
        st.header("üõ†Ô∏è 9 V≈® KH√ç")
        opt_audio_script = st.checkbox("Podcast Script", True)
        opt_video_script = st.checkbox("Video Script", False)
        opt_mindmap = st.checkbox("Mindmap (S∆° ƒë·ªì t∆∞ duy)", True)
        opt_report = st.checkbox("Deep Report", False)
        opt_flashcard = st.checkbox("Flashcards", False)
        opt_quiz = st.checkbox("Quiz (Tr·∫Øc nghi·ªám)", False)
        opt_infographic = st.checkbox("Infographic Data", False)
        opt_slides = st.checkbox("Slide Outline", False)
        opt_table = st.checkbox("Data Table", False)
        
        st.divider()
        if st.button("üóëÔ∏è X√≥a d·ªØ li·ªáu & L√†m m·ªõi"):
            st.session_state.chat_history = []
            st.session_state.gemini_files = []
            st.session_state.analysis_result = ""
            st.rerun()

    tab1, tab2 = st.tabs(["üìÇ Upload & 9 V≈© Kh√≠", "üí¨ Chat Chi Ti·∫øt"])

    # === TAB 1 ===
    with tab1:
        col_up, col_rec = st.columns(2)
        with col_up:
            st.subheader("1. Upload File")
            uploaded_files = st.file_uploader("Ch·ªçn file (mp3, wav, m4a)", type=['mp3', 'wav', 'm4a'], accept_multiple_files=True)
        with col_rec:
            st.subheader("2. Ghi √¢m tr·ª±c ti·∫øp")
            audio_bytes = audio_recorder()

        if st.button("üî• K√çCH HO·∫†T PH√ÇN T√çCH (9 V≈® KH√ç)", type="primary"):
            temp_paths = []
            if uploaded_files:
                for up_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
                        tmp.write(up_file.getvalue())
                        temp_paths.append(tmp.name)
            if audio_bytes:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                    tmp.write(audio_bytes)
                    temp_paths.append(tmp.name)
            
            if not temp_paths:
                st.warning("Ch∆∞a c√≥ file n√†o ƒë·ªÉ x·ª≠ l√Ω!")
            else:
                with st.spinner(f"ƒêang x·ª≠ l√Ω {len(temp_paths)} file v·ªõi {model_version}..."):
                    try:
                        gemini_files_objs = []
                        for path in temp_paths:
                            g_file = upload_to_gemini(path)
                            gemini_files_objs.append(g_file)
                            os.remove(path)
                        
                        st.session_state.gemini_files = gemini_files_objs
                        
                        prompt = "B·∫°n l√† chuy√™n gia NotebookLM. Ph√¢n t√≠ch file √¢m thanh v√† t·∫°o n·ªôi dung sau (ch·ªâ m·ª•c ƒë∆∞·ª£c ch·ªçn):\n"
                        if opt_audio_script: prompt += "- PODCAST SCRIPT: K·ªãch b·∫£n ƒë·ªëi tho·∫°i Host/Guest.\n"
                        if opt_video_script: prompt += "- VIDEO SCRIPT: K·ªãch b·∫£n video 2 c·ªôt.\n"
                        if opt_mindmap: prompt += "- MINDMAP: M√£ code Mermaid.js (graph TD) trong block ```mermaid```.\n"
                        if opt_report: prompt += "- DEEP REPORT: B√°o c√°o chuy√™n s√¢u.\n"
                        if opt_flashcard: prompt += "- FLASHCARDS: 5-10 th·∫ª ghi nh·ªõ.\n"
                        if opt_quiz: prompt += "- QUIZ: 5 c√¢u tr·∫Øc nghi·ªám c√≥ gi·∫£i th√≠ch.\n"
                        if opt_infographic: prompt += "- INFOGRAPHIC DATA: S·ªë li·ªáu/ƒêi·ªÉm nh·∫•n.\n"
                        if opt_slides: prompt += "- SLIDE OUTLINE: D√†n √Ω thuy·∫øt tr√¨nh.\n"
                        if opt_table: prompt += "- DATA TABLE: B·∫£ng d·ªØ li·ªáu Markdown.\n"

                        # --- LOGIC X·ª¨ L√ù L·ªñI (THEO L·ª∞A CH·ªåN C·ª¶A B√ÅC) ---
                        try:
                            model = genai.GenerativeModel(model_version)
                            response = model.generate_content([prompt] + gemini_files_objs)
                            st.session_state.analysis_result = response.text
                            st.success("‚úÖ X·ª≠ l√Ω xong!")
                        
                        except Exception as e:
                            # TR∆Ø·ªúNG H·ª¢P 1: AUTO FALLBACK
                            if "Auto Fallback" in fallback_mode:
                                st.warning(f"‚ö†Ô∏è Model {model_version} g·∫∑p l·ªói. ƒêang t·ª± ƒë·ªông chuy·ªÉn sang 'gemini-1.5-flash' ƒë·ªÉ c·ª©u v√£n t√¨nh th·∫ø...")
                                backup_model = genai.GenerativeModel("gemini-1.5-flash")
                                response = backup_model.generate_content([prompt] + gemini_files_objs)
                                st.session_state.analysis_result = response.text
                                st.success("‚úÖ X·ª≠ l√Ω xong (b·∫±ng Model d·ª± ph√≤ng)!")
                            
                            # TR∆Ø·ªúNG H·ª¢P 2: MANUAL STOP
                            else:
                                st.error(f"‚ùå Model {model_version} kh√¥ng ch·∫°y ƒë∆∞·ª£c! (L·ªói: {e})")
                                st.info("üîç ƒêang qu√©t danh s√°ch Model kh·∫£ d·ª•ng cho t√†i kho·∫£n c·ªßa b·∫°n...")
                                valid_models = get_valid_models()
                                if valid_models:
                                    st.write("‚úÖ **C√°c Model b·∫°n c√≥ th·ªÉ d√πng ngay l√∫c n√†y:**")
                                    st.code("\n".join(valid_models))
                                    st.warning("üëâ H√£y ch·ªçn m·ªôt trong c√°c model tr√™n ·ªü Sidebar v√† b·∫•m X·ª≠ l√Ω l·∫°i.")
                                else:
                                    st.error("Kh√¥ng t√¨m th·∫•y model n√†o kh·∫£ d·ª•ng. Ki·ªÉm tra l·∫°i API Key.")
                                st.stop() # D·ª´ng ch∆∞∆°ng tr√¨nh t·∫°i ƒë√¢y
                        # -------------------------------------------------------

                    except Exception as e:
                        st.error(f"L·ªói h·ªá th·ªëng: {e}")

        if st.session_state.analysis_result:
            st.divider()
            content = st.session_state.analysis_result
            if "```mermaid" in content:
                st.subheader("üß† B·∫£n ƒë·ªì t∆∞ duy")
                try:
                    mermaid_code = content.split("```mermaid")[1].split("```")[0]
                    st_mermaid(mermaid_code, height=500)
                except: pass
            st.markdown(content)
            
            doc = create_docx(content)
            doc_io = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            doc.save(doc_io.name)
            with open(doc_io.name, "rb") as f:
                st.download_button("üì• T·∫£i b√°o c√°o (.docx)", f, "NotebookLM_Ultimate.docx")
            os.remove(doc_io.name)

    # === TAB 2 ===
    with tab2:
        st.header("üí¨ Chat v·ªõi n·ªôi dung ghi √¢m")
        if not st.session_state.gemini_files:
            st.info("üëà Vui l√≤ng Upload v√† b·∫•m 'K√≠ch ho·∫°t ph√¢n t√≠ch' ·ªü Tab 1 tr∆∞·ªõc.")
        else:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])
            
            if user_input := st.chat_input("H·ªèi chi ti·∫øt..."):
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                with st.chat_message("user"): st.markdown(user_input)
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang tr·∫£ l·ªùi..."):
                        try:
                            chat_model = genai.GenerativeModel("gemini-1.5-flash")
                            response = chat_model.generate_content(st.session_state.gemini_files + [f"Context: N·ªôi dung file ghi √¢m. Tr·∫£ l·ªùi: {user_input}"])
                            st.markdown(response.text)
                            st.session_state.chat_history.append({"role": "assistant", "content": response.text})
                        except Exception as e: st.error(f"L·ªói chat: {e}")

if __name__ == "__main__":
    main()
