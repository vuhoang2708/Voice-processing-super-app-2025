import streamlit as st
import google.generativeai as genai
from docx import Document
from streamlit_mermaid import st_mermaid
from audio_recorder_streamlit import audio_recorder
import tempfile
import os
import time

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="NotebookLM Ultimate", page_icon="üíé", layout="wide")
st.markdown("""<style>.stButton>button {width: 100%; border-radius: 8px; height: 3em; font-weight: bold;}</style>""", unsafe_allow_html=True)

# --- QU·∫¢N L√ù TR·∫†NG TH√ÅI ---
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "gemini_files" not in st.session_state: st.session_state.gemini_files = [] 
if "analysis_result" not in st.session_state: st.session_state.analysis_result = ""

# --- H√ÄM H·ªñ TR·ª¢ ---
def configure_genai():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return True
    except:
        st.error("üö® Ch∆∞a nh·∫≠p API Key trong Secrets!")
        return False

def get_real_models():
    """H√†m l·∫•y danh s√°ch model TH·∫¨T t·ª´ t√†i kho·∫£n c·ªßa b√°c"""
    try:
        models = genai.list_models()
        valid_list = []
        for m in models:
            # Ch·ªâ l·∫•y model t·∫°o n·ªôi dung (b·ªè qua embedding) v√† ph·∫£i l√† d√≤ng Gemini
            if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name:
                valid_list.append(m.name)
        # S·∫Øp x·∫øp ƒë·ªÉ c√°c b·∫£n m·ªõi nh·∫•t (Flash/Pro) l√™n ƒë·∫ßu cho d·ªÖ ch·ªçn
        valid_list.sort(reverse=True) 
        return valid_list
    except:
        # N·∫øu l·ªói k·∫øt n·ªëi th√¨ tr·∫£ v·ªÅ danh s√°ch d·ª± ph√≤ng
        return ["models/gemini-1.5-flash", "models/gemini-1.5-pro"]

def upload_to_gemini(path, mime_type="audio/mp3"):
    file = genai.upload_file(path, mime_type=mime_type)
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    return file

def create_docx(content):
    doc = Document()
    doc.add_heading('NOTEBOOKLM ULTIMATE REPORT', 0)
    for line in content.split('\n'):
        if line.startswith('# '): doc.add_heading(line.replace('# ', ''), level=1)
        elif line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=2)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=3)
        else: doc.add_paragraph(line)
    return doc

# --- MAIN APP ---
def main():
    st.title("üíé NotebookLM Ultimate (Auto-Sync Models)")
    
    if not configure_genai(): return

    # --- SIDEBAR ---
    with st.sidebar:
        st.header("üß† Model Engine")
        
        # --- T·ª∞ ƒê·ªòNG T·∫¢I DANH S√ÅCH MODEL ---
        with st.spinner("ƒêang ƒë·ªìng b·ªô danh s√°ch Model..."):
            real_models = get_real_models()
        
        if not real_models:
            st.error("Kh√¥ng t√¨m th·∫•y model n√†o! Ki·ªÉm tra API Key.")
            return

        # Sidebar b√¢y gi·ªù s·∫Ω hi·ªán ƒë√∫ng nh·ªØng g√¨ Google cho ph√©p
        model_version = st.selectbox("Ch·ªçn Model (ƒê√£ ƒë·ªìng b·ªô):", real_models)
        # -----------------------------------
        
        st.divider()
        st.header("üõ†Ô∏è 9 V≈® KH√ç")
        opt_audio_script = st.checkbox("Podcast Script", True)
        opt_video_script = st.checkbox("Video Script", False)
        opt_mindmap = st.checkbox("Mindmap (S∆° ƒë·ªì t∆∞ duy)", True)
        opt_report = st.checkbox("Deep Report", False)
        opt_flashcard = st.checkbox("Flashcards", False)
        opt_quiz = st.checkbox("Quiz (Tr·∫Øc nghi·ªám)", False)
        opt_infographic = st.checkbox("Infographic Data", False)
        opt_slides = st.checkbox("Slide Outline", False)
        opt_table = st.checkbox("Data Table", False)
        
        st.divider()
        if st.button("üóëÔ∏è X√≥a d·ªØ li·ªáu & L√†m m·ªõi"):
            st.session_state.chat_history = []
            st.session_state.gemini_files = []
            st.session_state.analysis_result = ""
            st.rerun()

    tab1, tab2 = st.tabs(["üìÇ Upload & 9 V≈© Kh√≠", "üí¨ Chat Chi Ti·∫øt"])

    # === TAB 1 ===
    with tab1:
        col_up, col_rec = st.columns(2)
        with col_up:
            st.subheader("1. Upload File")
            uploaded_files = st.file_uploader("Ch·ªçn file (mp3, wav, m4a)", type=['mp3', 'wav', 'm4a'], accept_multiple_files=True)
        with col_rec:
            st.subheader("2. Ghi √¢m tr·ª±c ti·∫øp")
            audio_bytes = audio_recorder()

        if st.button("üî• K√çCH HO·∫†T PH√ÇN T√çCH (9 V≈® KH√ç)", type="primary"):
            temp_paths = []
            if uploaded_files:
                for up_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
                        tmp.write(up_file.getvalue())
                        temp_paths.append(tmp.name)
            if audio_bytes:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                    tmp.write(audio_bytes)
                    temp_paths.append(tmp.name)
            
            if not temp_paths:
                st.warning("Ch∆∞a c√≥ file n√†o ƒë·ªÉ x·ª≠ l√Ω!")
            else:
                with st.spinner(f"ƒêang x·ª≠ l√Ω {len(temp_paths)} file v·ªõi {model_version}..."):
                    try:
                        gemini_files_objs = []
                        for path in temp_paths:
                            g_file = upload_to_gemini(path)
                            gemini_files_objs.append(g_file)
                            os.remove(path)
                        
                        st.session_state.gemini_files = gemini_files_objs
                        
                        prompt = "B·∫°n l√† chuy√™n gia NotebookLM. Ph√¢n t√≠ch file √¢m thanh v√† t·∫°o n·ªôi dung sau (ch·ªâ m·ª•c ƒë∆∞·ª£c ch·ªçn):\n"
                        if opt_audio_script: prompt += "- PODCAST SCRIPT: K·ªãch b·∫£n ƒë·ªëi tho·∫°i Host/Guest.\n"
                        if opt_video_script: prompt += "- VIDEO SCRIPT: K·ªãch b·∫£n video 2 c·ªôt.\n"
                        if opt_mindmap: prompt += "- MINDMAP: M√£ code Mermaid.js (graph TD) trong block ```mermaid```.\n"
                        if opt_report: prompt += "- DEEP REPORT: B√°o c√°o chuy√™n s√¢u.\n"
                        if opt_flashcard: prompt += "- FLASHCARDS: 5-10 th·∫ª ghi nh·ªõ.\n"
                        if opt_quiz: prompt += "- QUIZ: 5 c√¢u tr·∫Øc nghi·ªám c√≥ gi·∫£i th√≠ch.\n"
                        if opt_infographic: prompt += "- INFOGRAPHIC DATA: S·ªë li·ªáu/ƒêi·ªÉm nh·∫•n.\n"
                        if opt_slides: prompt += "- SLIDE OUTLINE: D√†n √Ω thuy·∫øt tr√¨nh.\n"
                        if opt_table: prompt += "- DATA TABLE: B·∫£ng d·ªØ li·ªáu Markdown.\n"

                        # G·ªçi ƒë√∫ng c√°i t√™n model v·ª´a l·∫•y ƒë∆∞·ª£c t·ª´ list
                        model = genai.GenerativeModel(model_version)
                        response = model.generate_content([prompt] + gemini_files_objs)
                        
                        st.session_state.analysis_result = response.text
                        st.success("‚úÖ X·ª≠ l√Ω xong!")
                    except Exception as e:
                        st.error(f"L·ªói: {e}")

        if st.session_state.analysis_result:
            st.divider()
            content = st.session_state.analysis_result
            if "```mermaid" in content:
                st.subheader("üß† B·∫£n ƒë·ªì t∆∞ duy")
                try:
                    mermaid_code = content.split("```mermaid")[1].split("```")[0]
                    st_mermaid(mermaid_code, height=500)
                except: pass
            st.markdown(content)
            
            doc = create_docx(content)
            doc_io = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            doc.save(doc_io.name)
            with open(doc_io.name, "rb") as f:
                st.download_button("üì• T·∫£i b√°o c√°o (.docx)", f, "NotebookLM_Ultimate.docx")
            os.remove(doc_io.name)

    # === TAB 2 ===
    with tab2:
        st.header("üí¨ Chat v·ªõi n·ªôi dung ghi √¢m")
        if not st.session_state.gemini_files:
            st.info("üëà Vui l√≤ng Upload v√† b·∫•m 'K√≠ch ho·∫°t ph√¢n t√≠ch' ·ªü Tab 1 tr∆∞·ªõc.")
        else:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])
            
            if user_input := st.chat_input("H·ªèi chi ti·∫øt..."):
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                with st.chat_message("user"): st.markdown(user_input)
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang tr·∫£ l·ªùi..."):
                        try:
                            # Chat d√πng lu√¥n model ƒëang ch·ªçn cho ƒë·ªìng b·ªô
                            chat_model = genai.GenerativeModel(model_version)
                            response = chat_model.generate_content(st.session_state.gemini_files + [f"Context: N·ªôi dung file ghi √¢m. Tr·∫£ l·ªùi: {user_input}"])
                            st.markdown(response.text)
                            st.session_state.chat_history.append({"role": "assistant", "content": response.text})
                        except Exception as e: st.error(f"L·ªói chat: {e}")

if __name__ == "__main__":
    main()
