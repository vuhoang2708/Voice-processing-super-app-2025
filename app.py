import streamlit as st
import google.generativeai as genai
from docx import Document
from streamlit_mermaid import st_mermaid
from streamlit_audio_recorder import st_audio_recorder
import tempfile
import os
import time

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="NotebookLM Ultimate", page_icon="üíé", layout="wide")
st.markdown("""<style>.stButton>button {width: 100%; border-radius: 8px; height: 3em; font-weight: bold;}</style>""", unsafe_allow_html=True)

# --- QU·∫¢N L√ù TR·∫†NG TH√ÅI (SESSION STATE) ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "gemini_files" not in st.session_state:
    st.session_state.gemini_files = [] 
if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = ""

# --- H√ÄM H·ªñ TR·ª¢ ---
def configure_genai():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        return True
    except:
        st.error("üö® Ch∆∞a nh·∫≠p API Key trong Secrets!")
        return False

def upload_to_gemini(path, mime_type="audio/mp3"):
    file = genai.upload_file(path, mime_type=mime_type)
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    return file

def create_docx(content):
    doc = Document()
    doc.add_heading('NOTEBOOKLM ULTIMATE REPORT', 0)
    for line in content.split('\n'):
        if line.startswith('# '): doc.add_heading(line.replace('# ', ''), level=1)
        elif line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=2)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=3)
        else: doc.add_paragraph(line)
    return doc

# --- MAIN APP ---
def main():
    st.title("üíé NotebookLM Ultimate (9 Weapons + Chat + Live)")
    
    if not configure_genai(): return

    # --- SIDEBAR ---
    with st.sidebar:
        st.header("üß† Model Engine")
        model_version = st.selectbox(
            "Ch·ªçn Model:",
            ("gemini-1.5-flash", "gemini-2.0-flash-exp", "gemini-1.5-pro")
        )
        
        st.divider()
        st.header("üõ†Ô∏è 9 V≈® KH√ç (Cho Tab Ph√¢n t√≠ch)")
        
        st.caption("1. Nghe & Nh√¨n")
        opt_audio_script = st.checkbox("Podcast Script", True)
        opt_video_script = st.checkbox("Video Script", False)
        
        st.caption("2. H·ªá th·ªëng h√≥a")
        opt_mindmap = st.checkbox("Mindmap (S∆° ƒë·ªì t∆∞ duy)", True)
        opt_report = st.checkbox("Deep Report", False)
        
        st.caption("3. H·ªçc t·∫≠p")
        opt_flashcard = st.checkbox("Flashcards", False)
        opt_quiz = st.checkbox("Quiz (Tr·∫Øc nghi·ªám)", False)
        
        st.caption("4. D·ªØ li·ªáu")
        opt_infographic = st.checkbox("Infographic Data", False)
        opt_slides = st.checkbox("Slide Outline", False)
        opt_table = st.checkbox("Data Table", False)
        
        st.divider()
        if st.button("üóëÔ∏è X√≥a d·ªØ li·ªáu & L√†m m·ªõi"):
            st.session_state.chat_history = []
            st.session_state.gemini_files = []
            st.session_state.analysis_result = ""
            st.rerun()

    # --- GIAO DI·ªÜN TAB ---
    tab1, tab2 = st.tabs(["üìÇ Upload & 9 V≈© Kh√≠", "üí¨ Chat Chi Ti·∫øt"])

    # ================= TAB 1: UPLOAD & PH√ÇN T√çCH =================
    with tab1:
        col_up, col_rec = st.columns(2)
        files_to_process = []
        
        with col_up:
            st.subheader("1. Upload File (Nhi·ªÅu file)")
            uploaded_files = st.file_uploader("Ch·ªçn file (mp3, wav, m4a)", type=['mp3', 'wav', 'm4a'], accept_multiple_files=True)
        
        with col_rec:
            st.subheader("2. Ghi √¢m tr·ª±c ti·∫øp")
            audio_bytes = st_audio_recorder()

        if st.button("üî• K√çCH HO·∫†T PH√ÇN T√çCH (9 V≈® KH√ç)", type="primary"):
            # Gom file
            temp_paths = []
            if uploaded_files:
                for up_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
                        tmp.write(up_file.getvalue())
                        temp_paths.append(tmp.name)
            if audio_bytes:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                    tmp.write(audio_bytes)
                    temp_paths.append(tmp.name)
            
            if not temp_paths:
                st.warning("Ch∆∞a c√≥ file n√†o ƒë·ªÉ x·ª≠ l√Ω!")
            else:
                with st.spinner(f"ƒêang upload {len(temp_paths)} file l√™n Google & X·ª≠ l√Ω..."):
                    try:
                        gemini_files_objs = []
                        for path in temp_paths:
                            g_file = upload_to_gemini(path)
                            gemini_files_objs.append(g_file)
                            os.remove(path)
                        
                        st.session_state.gemini_files = gemini_files_objs
                        
                        prompt = "B·∫°n l√† chuy√™n gia NotebookLM. Ph√¢n t√≠ch c√°c file √¢m thanh v√† t·∫°o n·ªôi dung sau (ch·ªâ m·ª•c ƒë∆∞·ª£c ch·ªçn):\n"
                        if opt_audio_script: prompt += "- PODCAST SCRIPT: K·ªãch b·∫£n ƒë·ªëi tho·∫°i Host/Guest.\n"
                        if opt_video_script: prompt += "- VIDEO SCRIPT: K·ªãch b·∫£n video 2 c·ªôt.\n"
                        if opt_mindmap: prompt += "- MINDMAP: M√£ code Mermaid.js (graph TD) trong block ```mermaid```.\n"
                        if opt_report: prompt += "- DEEP REPORT: B√°o c√°o chuy√™n s√¢u.\n"
                        if opt_flashcard: prompt += "- FLASHCARDS: 5-10 th·∫ª ghi nh·ªõ.\n"
                        if opt_quiz: prompt += "- QUIZ: 5 c√¢u tr·∫Øc nghi·ªám c√≥ gi·∫£i th√≠ch.\n"
                        if opt_infographic: prompt += "- INFOGRAPHIC DATA: S·ªë li·ªáu/ƒêi·ªÉm nh·∫•n.\n"
                        if opt_slides: prompt += "- SLIDE OUTLINE: D√†n √Ω thuy·∫øt tr√¨nh.\n"
                        if opt_table: prompt += "- DATA TABLE: B·∫£ng d·ªØ li·ªáu Markdown.\n"

                        model = genai.GenerativeModel(model_version)
                        response = model.generate_content([prompt] + gemini_files_objs)
                        
                        st.session_state.analysis_result = response.text
                        st.success("‚úÖ X·ª≠ l√Ω xong! Chuy·ªÉn sang Tab Chat ƒë·ªÉ h·ªèi th√™m n·∫øu c·∫ßn.")

                    except Exception as e:
                        st.error(f"L·ªói: {e}")

        if st.session_state.analysis_result:
            st.divider()
            content = st.session_state.analysis_result
            
            if "```mermaid" in content:
                st.subheader("üß† B·∫£n ƒë·ªì t∆∞ duy")
                try:
                    mermaid_code = content.split("```mermaid")[1].split("```")[0]
                    st_mermaid(mermaid_code, height=500)
                except: pass
            
            st.markdown(content)
            
            doc = create_docx(content)
            doc_io = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            doc.save(doc_io.name)
            with open(doc_io.name, "rb") as f:
                st.download_button("üì• T·∫£i b√°o c√°o (.docx)", f, "NotebookLM_Ultimate.docx")
            os.remove(doc_io.name)

    # ================= TAB 2: CHAT V·ªöI D·ªÆ LI·ªÜU =================
    with tab2:
        st.header("üí¨ Chat v·ªõi n·ªôi dung ghi √¢m")
        if not st.session_state.gemini_files:
            st.info("üëà Vui l√≤ng Upload v√† b·∫•m 'K√≠ch ho·∫°t ph√¢n t√≠ch' ·ªü Tab 1 tr∆∞·ªõc.")
        else:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
            
            if user_input := st.chat_input("H·ªèi chi ti·∫øt v·ªÅ cu·ªôc h·ªçp..."):
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                with st.chat_message("user"):
                    st.markdown(user_input)
                
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang nghe l·∫°i bƒÉng ghi √¢m..."):
                        try:
                            chat_model = genai.GenerativeModel("gemini-1.5-flash")
                            response = chat_model.generate_content(
                                st.session_state.gemini_files + 
                                [f"Context: ƒê√¢y l√† n·ªôi dung c√°c file ghi √¢m. H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {user_input}"]
                            )
                            st.markdown(response.text)
                            st.session_state.chat_history.append({"role": "assistant", "content": response.text})
                        except Exception as e:
                            st.error(f"L·ªói chat: {e}")

if __name__ == "__main__":
    main()
