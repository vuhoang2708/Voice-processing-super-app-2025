import streamlit as st
import google.generativeai as genai
from docx import Document
from streamlit_mermaid import st_mermaid
from audio_recorder_streamlit import audio_recorder
import tempfile
import os
import time
import mimetypes
import re
import random
from collections import Counter

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Universal AI Studio (Smart Retry)", page_icon="üß†", layout="wide")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px; height: 3em; font-weight: bold; background: #1e3c72; color: white;}
    .stExpander {border: 1px solid #e0e0e0; border-radius: 8px; margin-bottom: 10px; background-color: #ffffff;}
    .stMarkdown h2 {color: #1a2a6c; border-bottom: 2px solid #eee; padding-bottom: 5px;}
    div[data-testid="stButton"] > button:contains("D·ª™NG") {background-color: #d32f2f !important;}
</style>
""", unsafe_allow_html=True)

# --- BI·∫æN TO√ÄN C·ª§C ---
STRICT_RULES = "CH·ªà D√ôNG FILE G·ªêC. C·∫§M B·ªäA T√äN DI·ªÑN GI·∫¢. C·∫§M B·ªäA N·ªòI DUNG. TR√çCH D·∫™N GI·ªú [mm:ss]."

# --- QU·∫¢N L√ù SESSION ---
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "gemini_files" not in st.session_state: st.session_state.gemini_files = [] 
if "analysis_result" not in st.session_state: st.session_state.analysis_result = ""
if "is_auto_running" not in st.session_state: st.session_state.is_auto_running = False
if "loop_count" not in st.session_state: st.session_state.loop_count = 0

# --- H√ÄM H·ªñ TR·ª¢ ---
def configure_genai(user_key=None):
    api_key = user_key or st.secrets.get("GOOGLE_API_KEY") or (random.choice(st.secrets["SYSTEM_KEYS"]) if "SYSTEM_KEYS" in st.secrets else None)
    if not api_key: return False
    try:
        genai.configure(api_key=api_key)
        return True
    except: return False

def get_optimized_models():
    try:
        models = genai.list_models()
        valid = [m.name for m in models if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name]
        priority = ["gemini-3-flash-preview", "gemini-2.0-flash-exp", "gemini-1.5-flash"]
        final_list = []
        for p in priority:
            found = [m for m in valid if p in m]
            for f in found:
                if f not in final_list: final_list.append(f)
        for v in valid:
            if v not in final_list: final_list.append(v)
        return final_list if final_list else ["models/gemini-1.5-flash"]
    except: return ["models/gemini-1.5-flash"]

def upload_to_gemini(path):
    mime_type, _ = mimetypes.guess_type(path)
    file = genai.upload_file(path, mime_type=mime_type or "application/octet-stream")
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    return file

def create_docx(content):
    doc = Document()
    doc.add_heading('B√ÅO C√ÅO PH√ÇN T√çCH AI', 0)
    clean_content = re.sub(r'<[^>]+>', '', content)
    for line in clean_content.split('\n'):
        if line.startswith('# '): doc.add_heading(line.replace('# ', ''), level=1)
        elif line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=2)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=3)
        else: doc.add_paragraph(line)
    return doc

def check_is_looping(text):
    """H√†m ph√°t hi·ªán AI b·ªã k·∫πt ƒëƒ©a (L·∫∑p l·∫°i 1 c√¢u qu√° nhi·ªÅu l·∫ßn)"""
    if not text: return False
    lines = text.split('\n')
    # N·∫øu 1 d√≤ng xu·∫•t hi·ªán qu√° 5 l·∫ßn -> Nghi v·∫•n l·∫∑p
    if len(lines) > 10:
        counts = Counter([line.strip() for line in lines if len(line.strip()) > 10])
        if any(c > 5 for c in counts.values()): return True
    # N·∫øu 1 c·ª•m t·ª´ ng·∫Øn l·∫∑p l·∫°i li√™n t·ª•c (VD: Forum ·∫•y Forum ·∫•y)
    if len(text) > 500:
        sample = text[-500:]
        words = sample.split()
        if len(words) > 20 and len(set(words)) < 5: return True
    return False

def generate_with_smart_retry(model_name, prompt, files, base_temp=0.1):
    """C∆° ch·∫ø th·ª≠ l·∫°i th√¥ng minh: TƒÉng nhi·ªát ƒë·ªô n·∫øu b·ªã l·∫∑p"""
    attempts = 0
    max_attempts = 3
    current_temp = base_temp
    
    while attempts < max_attempts:
        try:
            # C·∫•u h√¨nh nhi·ªát ƒë·ªô ƒë·ªông
            config = genai.types.GenerationConfig(max_output_tokens=8192, temperature=current_temp)
            model = genai.GenerativeModel(model_name)
            
            response = model.generate_content([prompt] + files, generation_config=config)
            text = response.text
            
            # Ki·ªÉm tra xem c√≥ b·ªã l·∫∑p kh√¥ng
            if check_is_looping(text):
                st.toast(f"‚ö†Ô∏è Ph√°t hi·ªán l·∫∑p t·ª´ (Temp {current_temp}). ƒêang th·ª≠ l·∫°i v·ªõi ƒë·ªô s√°ng t·∫°o cao h∆°n...", icon="üîÑ")
                current_temp += 0.2 # TƒÉng nhi·ªát ƒë·ªô ƒë·ªÉ tho√°t l·∫∑p
                attempts += 1
                continue
            
            return text # N·∫øu ngon th√¨ tr·∫£ v·ªÅ lu√¥n
            
        except Exception as e:
            if "429" in str(e): # L·ªói Quota th√¨ th·ª≠ model kh√°c (Fallback)
                st.toast("‚ö†Ô∏è Model qu√° t·∫£i, ƒëang chuy·ªÉn sang d·ª± ph√≤ng...", icon="üîÄ")
                return generate_with_fallback_model(prompt, files)
            else:
                st.error(f"L·ªói: {e}")
                return None
    
    return text # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ªët nh·∫•t c√≥ th·ªÉ sau 3 l·∫ßn th·ª≠

def generate_with_fallback_model(prompt, files):
    """H√†m ph·ª•: Ch·∫°y model d·ª± ph√≤ng (1.5 Flash)"""
    try:
        model = genai.GenerativeModel("models/gemini-1.5-flash")
        res = model.generate_content([prompt] + files)
        return res.text
    except: return "L·ªói h·ªá th·ªëng: Kh√¥ng th·ªÉ x·ª≠ l√Ω."

# --- MAIN APP ---
def main():
    st.title("üõ°Ô∏è Universal AI Studio (Smart Retry)")
    
    with st.sidebar:
        st.header("üéØ CH·∫æ ƒê·ªò HO·∫†T ƒê·ªòNG")
        main_mode = st.radio("M·ª•c ti√™u ch√≠nh:", ("üìù G·ª° bƒÉng nguy√™n vƒÉn", "üìä Ph√¢n t√≠ch chuy√™n s√¢u"))
        
        if main_mode == "üìä Ph√¢n t√≠ch chuy√™n s√¢u":
            st.subheader("CH·ªåN V≈® KH√ç:")
            opt_summary = st.checkbox("üìã T√≥m t·∫Øt n·ªôi dung", True)
            opt_action = st.checkbox("‚úÖ Danh s√°ch H√†nh ƒë·ªông", True)
            opt_process = st.checkbox("üîÑ Tr√≠ch xu·∫•t Quy tr√¨nh", False)
            opt_prosody = st.checkbox("üé≠ Ph√¢n t√≠ch C·∫£m x√∫c", False)
            opt_mindmap = st.checkbox("üß† V·∫Ω S∆° ƒë·ªì t∆∞ duy", True)
            opt_quiz = st.checkbox("‚ùì C√¢u h·ªèi Tr·∫Øc nghi·ªám", False)
            opt_slides = st.checkbox("üñ•Ô∏è D√†n √Ω Slide", False)
        else:
            st.info("üí° Ch·∫ø ƒë·ªô G·ª° bƒÉng s·∫Ω t·ª± ƒë·ªông ch·∫°y n·ªëi ti·∫øp khi h·∫øt token.")
            auto_continue = st.checkbox("T·ª± ƒë·ªông n·ªëi ƒëo·∫°n (Auto-Continue)", value=True)
        
        st.divider()
        with st.expander("‚öôÔ∏è C·∫•u h√¨nh & Key"):
            user_key = st.text_input("Nh·∫≠p Key ri√™ng:", type="password")
            if configure_genai(user_key):
                st.success("ƒê√£ k·∫øt n·ªëi!")
                models = get_optimized_models()
                model_version = st.selectbox("Engine:", models, index=0)
                if main_mode.startswith("üìä"):
                    detail_level = st.select_slider("ƒê·ªô chi ti·∫øt:", ["S∆° l∆∞·ª£c", "Ti√™u chu·∫©n", "S√¢u"], value="S√¢u")
            else: st.error("Ch∆∞a k·∫øt n·ªëi!")

        if st.button("üóëÔ∏è Reset App"):
            st.session_state.clear(); st.rerun()

    # --- TABS ---
    tab_work, tab_chat = st.tabs(["üìÇ X·ª≠ l√Ω D·ªØ li·ªáu", "üí¨ Chat"])

    with tab_work:
        if not st.session_state.is_auto_running:
            up_files = st.file_uploader("Upload file", accept_multiple_files=True)
            audio_bytes = audio_recorder()

            if st.button("üöÄ B·∫ÆT ƒê·∫¶U TH·ª∞C THI", type="primary"):
                temp_paths = []
                if up_files:
                    for f in up_files:
                        ext = os.path.splitext(f.name)[1] or ".txt"
                        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
                            tmp.write(f.getvalue()); temp_paths.append(tmp.name)
                if audio_bytes:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                        tmp.write(audio_bytes); temp_paths.append(tmp.name)
                
                if not temp_paths:
                    st.warning("Ch∆∞a c√≥ file!")
                else:
                    with st.spinner(f"ƒêang x·ª≠ l√Ω (C∆° ch·∫ø Smart Retry ƒëang b·∫≠t)..."):
                        try:
                            g_files = [upload_to_gemini(p) for p in temp_paths]
                            st.session_state.gemini_files = g_files
                            
                            if main_mode.startswith("üìù"):
                                prompt = f"""
                                {STRICT_RULES}
                                NHI·ªÜM V·ª§: G·ª° bƒÉng NGUY√äN VƒÇN 100%.
                                Y√äU C·∫¶U:
                                - C·ªë g·∫Øng nghe k·ªπ t·ª´ng t·ª´.
                                - N·∫øu g·∫∑p ƒëo·∫°n nhi·ªÖu/l·∫∑p: H√£y th·ª≠ ph√¢n t√≠ch l·∫°i. N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c th√¨ ghi [ƒêO·∫†N NHI·ªÑU - KH√îNG R√ï] r·ªìi ƒëi ti·∫øp.
                                - KH√îNG ƒê∆Ø·ª¢C B·ªé QUA NGAY L·∫¨P T·ª®C.
                                - ƒê·ªãnh danh: 'Di·ªÖn gi·∫£'.
                                """
                                if auto_continue:
                                    st.session_state.is_auto_running = True
                                    st.session_state.loop_count = 1
                                
                                # G·ªåI H√ÄM SMART RETRY (Temp b·∫Øt ƒë·∫ßu t·ª´ 0.1)
                                res_text = generate_with_smart_retry(model_version, prompt, g_files, base_temp=0.1)
                            else:
                                prompt = f"{STRICT_RULES}\nNHI·ªÜM V·ª§: Ph√¢n t√≠ch s√¢u {detail_level} cho c√°c m·ª•c:\n"
                                if opt_summary: prompt += "## 1. T√ìM T·∫ÆT CHI TI·∫æT\n"
                                if opt_action: prompt += "## 2. H√ÄNH ƒê·ªòNG C·∫¶N L√ÄM\n"
                                if opt_process: prompt += "## 3. QUY TR√åNH CHI TI·∫æT\n"
                                if opt_prosody: prompt += "## 4. PH√ÇN T√çCH C·∫¢M X√öC\n"
                                if opt_mindmap: prompt += "## 5. M√É S∆† ƒê·ªí T∆Ø DUY (Mermaid)\n"
                                if opt_quiz: prompt += "## 6. C√ÇU H·ªéI TR·∫ÆC NGHI·ªÜM\n"
                                if opt_slides: prompt += "## 7. D√ÄN √ù SLIDE\n"
                                
                                # Ph√¢n t√≠ch th√¨ Temp cao h∆°n ch√∫t (0.3)
                                res_text = generate_with_smart_retry(model_version, prompt, g_files, base_temp=0.3)

                            st.session_state.analysis_result = res_text
                            st.rerun()
                        except Exception as e: st.error(f"L·ªói: {e}")

        # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        if st.session_state.analysis_result:
            if st.session_state.is_auto_running:
                st.warning(f"üîÑ ƒêang t·ª± ƒë·ªông g·ª° bƒÉng ƒëo·∫°n ti·∫øp theo (V√≤ng l·∫∑p #{st.session_state.loop_count})...")
                if st.button("üõë D·ª™NG L·∫†I NGAY"):
                    st.session_state.is_auto_running = False
                    st.success("ƒê√£ d·ª´ng.")
                    st.rerun()

            st.divider()
            res = st.session_state.analysis_result
            
            # Hi·ªÉn th·ªã (L·ªçc r√°c)
            clean_res = "\n".join([line for line in res.split('\n') if not line.strip().startswith(('*', 'Wait,', 'Refining', 'Final check', 'Constraint'))])
            
            sections = clean_res.split("## ")
            for s in sections:
                if not s.strip(): continue
                lines = s.split("\n")
                with st.expander(f"üìå {lines[0].strip()}", expanded=True):
                    st.markdown("\n".join(lines[1:]))
            
            # Download
            doc = create_docx(res)
            doc_io = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            doc.save(doc_io.name)
            with open(doc_io.name, "rb") as f:
                st.download_button("üì• T·∫£i B√°o C√°o (.docx)", f, "Bao_Cao_AI.docx", type="primary")
            os.remove(doc_io.name)

            # --- LOGIC AUTO-CONTINUE (G·ª† BƒÇNG) ---
            if st.session_state.is_auto_running and main_mode.startswith("üìù"):
                st.divider()
                placeholder = st.empty()
                for i in range(3, 0, -1):
                    placeholder.info(f"‚è≥ Chu·∫©n b·ªã n·ªëi ƒëo·∫°n ti·∫øp theo trong {i} gi√¢y...")
                    time.sleep(1)
                placeholder.empty()
                
                with st.spinner(f"ü§ñ AI ƒëang nghe ti·∫øp ƒëo·∫°n {st.session_state.loop_count + 1}..."):
                    try:
                        last_part = res[-500:] 
                        c_prompt = f"""
                        CONTEXT: B·∫°n ƒëang g·ª° bƒÉng d·ªü dang file √¢m thanh n√†y.
                        M·ªé NEO (ƒêo·∫°n cu·ªëi c√πng b·∫°n v·ª´a vi·∫øt): "...{last_part}"
                        NHI·ªÜM V·ª§: T√¨m v·ªã tr√≠ M·ªé NEO, vi·∫øt ti·∫øp NGUY√äN VƒÇN ƒëo·∫°n sau. KH√îNG vi·∫øt l·∫°i m·ªè neo.
                        """
                        
                        # D√πng Smart Retry cho c·∫£ ƒëo·∫°n n·ªëi ti·∫øp
                        c_res_text = generate_with_smart_retry(model_version, c_prompt, st.session_state.gemini_files, base_temp=0.1)
                        
                        if not c_res_text or len(c_res_text) < 50 or "k·∫øt th√∫c" in c_res_text.lower():
                            st.session_state.is_auto_running = False
                            st.success("‚úÖ ƒê√£ g·ª° bƒÉng xong to√†n b·ªô file!")
                        else:
                            st.session_state.analysis_result += "\n\n" + c_res_text
                            st.session_state.loop_count += 1
                            st.rerun()
                            
                    except Exception as e:
                        st.error(f"L·ªói ho·∫∑c ƒë√£ h·∫øt file: {e}")
                        st.session_state.is_auto_running = False

    with tab_chat:
        st.header("üí¨ Chat")
        if st.session_state.gemini_files:
            for m in st.session_state.chat_history:
                with st.chat_message(m["role"]): st.markdown(m["content"])
            if inp := st.chat_input("H·ªèi AI..."):
                st.session_state.chat_history.append({"role": "user", "content": inp})
                with st.chat_message("user"): st.markdown(inp)
                with st.chat_message("assistant"):
                    try:
                        m_chat = genai.GenerativeModel(model_version)
                        r = m_chat.generate_content(st.session_state.gemini_files + [f"Tr·∫£ l·ªùi t·ª´ file: {inp}"])
                        st.markdown(r.text); st.session_state.chat_history.append({"role": "assistant", "content": r.text})
                    except Exception as e: st.error(f"L·ªói: {e}")
        else: st.info("üëà Upload file tr∆∞·ªõc.")

if __name__ == "__main__":
    main()
