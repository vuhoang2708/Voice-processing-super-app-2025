import streamlit as st
import google.generativeai as genai
from docx import Document
from streamlit_mermaid import st_mermaid
from audio_recorder_streamlit import audio_recorder
import tempfile
import os
import time
import mimetypes
import re
import random

# --- 1. C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Universal AI Studio (Ultimate)", page_icon="üíé", layout="wide")
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 8px; height: 3em; font-weight: bold; background: #1e3c72; color: white;}
    .stExpander {border: 1px solid #e0e0e0; border-radius: 8px; margin-bottom: 10px; background-color: #ffffff;}
    .stMarkdown h2 {color: #1a2a6c; border-bottom: 2px solid #eee; padding-bottom: 5px;}
    .error-box {padding: 15px; background-color: #ffebee; border: 1px solid #ffcdd2; border-radius: 5px; color: #c62828; margin-bottom: 10px;}
</style>
""", unsafe_allow_html=True)

# --- 2. BI·∫æN TO√ÄN C·ª§C ---
STRICT_RULES = "CH·ªà D√ôNG FILE G·ªêC. C·∫§M B·ªäA T√äN DI·ªÑN GI·∫¢. C·∫§M B·ªäA N·ªòI DUNG. TR√çCH D·∫™N GI·ªú [mm:ss]."

# --- 3. QU·∫¢N L√ù SESSION ---
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "gemini_files" not in st.session_state: st.session_state.gemini_files = [] 
if "analysis_result" not in st.session_state: st.session_state.analysis_result = ""
if "is_auto_running" not in st.session_state: st.session_state.is_auto_running = False
if "loop_count" not in st.session_state: st.session_state.loop_count = 0
# Bi·∫øn ph·ª•c v·ª• Retry
if "quota_error" not in st.session_state: st.session_state.quota_error = False
if "last_prompt" not in st.session_state: st.session_state.last_prompt = ""
if "last_config" not in st.session_state: st.session_state.last_config = None

# --- 4. H√ÄM H·ªñ TR·ª¢ ---
def get_system_key():
    try:
        if "SYSTEM_KEYS" in st.secrets:
            keys = st.secrets["SYSTEM_KEYS"]
            if isinstance(keys, str): 
                keys = [k.strip() for k in keys.replace('[','').replace(']','').replace('"','').replace("'",'').split(',')]
            return random.choice(keys)
        elif "GOOGLE_API_KEY" in st.secrets:
            return st.secrets["GOOGLE_API_KEY"]
    except: return None

def configure_genai(user_key=None):
    api_key = user_key if user_key else get_system_key()
    if not api_key: return False
    try:
        genai.configure(api_key=api_key)
        return True
    except: return False

def get_optimized_models():
    # Danh s√°ch c·ª©ng ƒë·ªÉ ƒë·∫£m b·∫£o lu√¥n c√≥ l·ª±a ch·ªçn
    return ["models/gemini-3.0-flash-preview", "models/gemini-2.0-flash-exp", "models/gemini-1.5-flash", "models/gemini-1.5-pro"]

def format_model_name(name):
    return name.replace("models/", "").replace("-preview", " (Pre)").replace("-latest", "").upper()

def upload_to_gemini(path):
    mime_type, _ = mimetypes.guess_type(path)
    file = genai.upload_file(path, mime_type=mime_type or "application/octet-stream")
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    return file

def create_docx(content):
    doc = Document()
    doc.add_heading('B√ÅO C√ÅO', 0)
    clean_content = content.replace("```markdown", "").replace("```", "")
    for line in clean_content.split('\n'):
        if line.startswith('# '): doc.add_heading(line.replace('# ', ''), level=1)
        elif line.startswith('## '): doc.add_heading(line.replace('## ', ''), level=2)
        elif line.startswith('### '): doc.add_heading(line.replace('### ', ''), level=3)
        else: doc.add_paragraph(line)
    return doc

def get_safe_response(response):
    try:
        finish_reason = response.candidates[0].finish_reason
        if finish_reason in [1, 2]: return response.text
        elif finish_reason == 3: return "\n\n[C·∫¢NH B√ÅO: N·ªôi dung b·ªã ch·∫∑n do Safety.]"
        elif finish_reason == 4: return "\n\n[D·ª™NG: Ph√°t hi·ªán n·ªôi dung c√≥ b·∫£n quy·ªÅn.]"
        else: return f"\n\n[L·ªói: Finish Reason {finish_reason}]"
    except: return response.text

# --- 5. MAIN APP ---
def main():
    st.title("üíé Universal AI Studio (Ultimate)")
    
    # --- SIDEBAR ---
    with st.sidebar:
        st.header("üéØ CH·∫æ ƒê·ªò")
        main_mode = st.radio("M·ª•c ti√™u:", ("üìù G·ª° bƒÉng nguy√™n vƒÉn", "üìä Ph√¢n t√≠ch chuy√™n s√¢u"))
        
        if main_mode == "üìä Ph√¢n t√≠ch chuy√™n s√¢u":
            st.subheader("KHO V≈® KH√ç (FULL):")
            
            st.markdown("**1. C·ªët l√µi**")
            opt_summary = st.checkbox("üìã T√≥m t·∫Øt & H√†nh ƒë·ªông", True)
            opt_process = st.checkbox("üîÑ Tr√≠ch xu·∫•t Quy tr√¨nh", False)
            opt_prosody = st.checkbox("üé≠ Ph√¢n t√≠ch C·∫£m x√∫c", False)
            opt_gossip = st.checkbox("‚òï Ch·∫ø ƒë·ªô 'B√† t√°m'", False)

            st.markdown("**2. S√°ng t·∫°o Nghe/Nh√¨n**")
            opt_podcast = st.checkbox("üéôÔ∏è K·ªãch b·∫£n Podcast", False)
            opt_video = st.checkbox("üé¨ K·ªãch b·∫£n Video", False)
            opt_mindmap = st.checkbox("üß† S∆° ƒë·ªì t∆∞ duy (Mindmap)", True)

            st.markdown("**3. H·ªçc t·∫≠p & Nghi√™n c·ª©u**")
            opt_report = st.checkbox("üìë B√°o c√°o chuy√™n s√¢u", False)
            opt_briefing = st.checkbox("üìÑ Briefing Doc (T√≥m l∆∞·ª£c)", False)
            opt_timeline = st.checkbox("‚è≥ Timeline (D√≤ng th·ªùi gian)", False)
            opt_faq = st.checkbox("‚ùì FAQ (H·ªèi ƒë√°p)", False)
            opt_quiz = st.checkbox("üìù Quiz & Th·∫ª nh·ªõ", False)
            
            st.markdown("**4. D·ªØ li·ªáu**")
            opt_slides = st.checkbox("üñ•Ô∏è D√†n √Ω Slide", False)
            opt_table = st.checkbox("üìâ B·∫£ng s·ªë li·ªáu", False)
        
        st.divider()
        
        with st.expander("‚öôÔ∏è C·∫•u h√¨nh & Key", expanded=True):
            initial_key = st.text_input("Key ri√™ng (T√πy ch·ªçn):", type="password")
            if configure_genai(initial_key):
                st.success("ƒê√£ k·∫øt n·ªëi!")
                models = get_optimized_models()
                model_version = st.selectbox("Engine:", models, index=0, format_func=format_model_name)
                if main_mode.startswith("üìä"):
                    detail_level = st.select_slider("Chi ti·∫øt:", ["S∆° l∆∞·ª£c", "Ti√™u chu·∫©n", "S√¢u"], value="S√¢u")
            else: st.error("Ch∆∞a k·∫øt n·ªëi!")

        if st.button("üóëÔ∏è Reset"):
            st.session_state.clear(); st.rerun()

    # --- X·ª¨ L√ù L·ªñI QUOTA (INTERACTIVE) ---
    if st.session_state.quota_error:
        st.markdown("""
        <div class="error-box">
            <h3>‚ö†Ô∏è H·∫æT H·∫†N M·ª®C (429 QUOTA EXCEEDED)</h3>
            <p>Model b·∫°n ch·ªçn ƒë√£ h·∫øt l∆∞·ª£t d√πng mi·ªÖn ph√≠. B·∫°n mu·ªën x·ª≠ l√Ω sao?</p>
        </div>
        """, unsafe_allow_html=True)
        
        c1, c2 = st.columns(2)
        with c1:
            rescue_key = st.text_input("üîë Nh·∫≠p API Key M·ªöI ƒë·ªÉ ti·∫øp t·ª•c:", type="password", key="rescue")
            if st.button("üöÄ Th·ª≠ l·∫°i v·ªõi Key n√†y"):
                if configure_genai(rescue_key):
                    st.session_state.quota_error = False
                    st.rerun() # Ch·∫°y l·∫°i v·ªõi key m·ªõi
        with c2:
            st.write("Ho·∫∑c:")
            if st.button("‚¨áÔ∏è H·∫° xu·ªëng 1.5 Flash (Mi·ªÖn ph√≠)"):
                st.session_state.quota_error = False
                with st.spinner("ƒêang chuy·ªÉn sang 1.5 Flash..."):
                    try:
                        # H·∫° c·∫•p model
                        model = genai.GenerativeModel("models/gemini-1.5-flash")
                        response = model.generate_content([st.session_state.last_prompt] + st.session_state.gemini_files, generation_config=st.session_state.last_config)
                        st.session_state.analysis_result = get_safe_response(response)
                        st.rerun()
                    except Exception as e: st.error(f"L·ªói: {e}")
        st.divider()

    # --- TABS ---
    tab_work, tab_chat = st.tabs(["üìÇ X·ª≠ l√Ω", "üí¨ Chat"])

    with tab_work:
        if not st.session_state.quota_error:
            up_files = st.file_uploader("Upload file", accept_multiple_files=True)
            audio_bytes = audio_recorder()

            if st.button("üöÄ B·∫ÆT ƒê·∫¶U", type="primary"):
                temp_paths = []
                if up_files:
                    for f in up_files:
                        ext = os.path.splitext(f.name)[1] or ".txt"
                        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
                            tmp.write(f.getvalue()); temp_paths.append(tmp.name)
                if audio_bytes:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                        tmp.write(audio_bytes); temp_paths.append(tmp.name)
                
                if not temp_paths:
                    st.warning("Ch∆∞a c√≥ file!")
                else:
                    with st.spinner(f"ƒêang x·ª≠ l√Ω v·ªõi {format_model_name(model_version)}..."):
                        try:
                            g_files = [upload_to_gemini(p) for p in temp_paths]
                            st.session_state.gemini_files = g_files
                            
                            # T·∫Øt b·ªô l·ªçc an to√†n
                            safety_settings = [
                                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                            ]
                            
                            gen_config = genai.types.GenerationConfig(max_output_tokens=8192, temperature=0.2)

                            if main_mode.startswith("üìù"):
                                prompt = f"""
                                {STRICT_RULES}
                                NHI·ªÜM V·ª§: G·ª° bƒÉng NGUY√äN VƒÇN 100%.
                                Y√äU C·∫¶U:
                                1. B·∫Øt ƒë·∫ßu m·ªói c√¢u b·∫±ng [Ph√∫t:Gi√¢y].
                                2. Vi·∫øt l·∫°i ch√≠nh x√°c t·ª´ng t·ª´.
                                3. ƒê·ªãnh danh: 'Ng∆∞·ªùi n√≥i 1', 'Ng∆∞·ªùi n√≥i 2'.
                                4. Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát.
                                """
                                # M·∫∂C ƒê·ªäNH B·∫¨T AUTO CONTINUE
                                st.session_state.is_auto_running = True
                                st.session_state.loop_count = 1
                            else:
                                # T·ªîNG H·ª¢P PROMPT FULL T√çNH NƒÇNG
                                prompt = f"{STRICT_RULES}\nNHI·ªÜM V·ª§: Ph√¢n t√≠ch s√¢u {detail_level} cho c√°c m·ª•c sau:\n"
                                if opt_summary: prompt += "## T√ìM T·∫ÆT & H√ÄNH ƒê·ªòNG\n"
                                if opt_process: prompt += "## QUY TR√åNH CHI TI·∫æT\n"
                                if opt_prosody: prompt += "## PH√ÇN T√çCH C·∫¢M X√öC\n"
                                if opt_gossip: prompt += "## G√ìC B√Ä T√ÅM\n"
                                if opt_podcast: prompt += "## K·ªäCH B·∫¢N PODCAST\n"
                                if opt_video: prompt += "## K·ªäCH B·∫¢N VIDEO\n"
                                if opt_mindmap: prompt += "## M√É S∆† ƒê·ªí T∆Ø DUY (Mermaid)\n"
                                if opt_report: prompt += "## B√ÅO C√ÅO CHUY√äN S√ÇU\n"
                                if opt_briefing: prompt += "## BRIEFING DOC\n"
                                if opt_timeline: prompt += "## TIMELINE S·ª∞ KI·ªÜN\n"
                                if opt_faq: prompt += "## C√ÇU H·ªéI TH∆Ø·ªúNG G·∫∂P (FAQ)\n"
                                if opt_quiz: prompt += "## TR·∫ÆC NGHI·ªÜM & TH·∫∫ NH·ªö\n"
                                if opt_slides: prompt += "## D√ÄN √ù SLIDE\n"
                                if opt_table: prompt += "## B·∫¢NG S·ªê LI·ªÜU\n"

                            # L∆ØU TR·∫†NG TH√ÅI ƒê·ªÇ RETRY
                            st.session_state.last_prompt = prompt
                            st.session_state.last_config = gen_config

                            model = genai.GenerativeModel(model_version)
                            response = model.generate_content(
                                [prompt] + g_files, 
                                generation_config=gen_config,
                                safety_settings=safety_settings
                            )
                            
                            st.session_state.analysis_result = get_safe_response(response)
                            st.rerun()
                            
                        except Exception as e:
                            err_msg = str(e)
                            if "429" in err_msg or "Quota" in err_msg:
                                st.session_state.quota_error = True
                                st.rerun()
                            elif "404" in err_msg or "Not Found" in err_msg:
                                st.toast(f"‚ö†Ô∏è Model {format_model_name(model_version)} l·ªói (404). T·ª± ƒë·ªông chuy·ªÉn sang 1.5 Flash...", icon="üîÑ")
                                try:
                                    fb_model = genai.GenerativeModel("models/gemini-1.5-flash")
                                    response = fb_model.generate_content([prompt] + g_files, generation_config=gen_config, safety_settings=safety_settings)
                                    st.session_state.analysis_result = get_safe_response(response)
                                    st.rerun()
                                except Exception as e2: st.error(f"L·ªói h·ªá th·ªëng: {e2}")
                            else:
                                st.error(f"L·ªói: {e}")

        # HI·ªÇN TH·ªä K·∫æT QU·∫¢
        if st.session_state.analysis_result:
            if st.session_state.is_auto_running:
                st.warning(f"üîÑ ƒêang t·ª± ƒë·ªông ch·∫°y ti·∫øp (V√≤ng {st.session_state.loop_count})...")
                if st.button("üõë D·ª™NG"):
                    st.session_state.is_auto_running = False
                    st.success("ƒê√£ d·ª´ng."); st.rerun()

            st.divider()
            res = st.session_state.analysis_result
            
            # X·ª≠ l√Ω Mindmap
            if "```mermaid" in res:
                try:
                    m_code = res.split("```mermaid")[1].split("```")[0]
                    st_mermaid(m_code, height=500)
                except: pass
            
            # Hi·ªÉn th·ªã Text
            sections = res.split("## ")
            for s in sections:
                if not s.strip(): continue
                lines = s.split("\n")
                with st.expander(f"üìå {lines[0].strip()}", expanded=True):
                    st.markdown("\n".join(lines[1:]))

            # Download
            doc = create_docx(res)
            doc_io = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
            doc.save(doc_io.name)
            with open(doc_io.name, "rb") as f:
                st.download_button("üì• T·∫£i B√°o C√°o", f, "Bao_Cao.docx", type="primary")
            os.remove(doc_io.name)

            # AUTO-CONTINUE (Logic c≈© + Retry Quota)
            if st.session_state.is_auto_running and main_mode.startswith("üìù"):
                if "[D·ª™NG:" in res or "[C·∫¢NH B√ÅO:" in res:
                    st.session_state.is_auto_running = False
                    st.error("‚ö†Ô∏è D·ª´ng do b·∫£n quy·ªÅn/an to√†n.")
                else:
                    st.divider()
                    placeholder = st.empty()
                    for i in range(3, 0, -1):
                        placeholder.info(f"‚è≥ Ch·∫°y ti·∫øp trong {i}s...")
                        time.sleep(1)
                    placeholder.empty()
                    
                    with st.spinner("ƒêang nghe ti·∫øp..."):
                        try:
                            cont_config = genai.types.GenerationConfig(max_output_tokens=8192, temperature=0.2)
                            model = genai.GenerativeModel(model_version)
                            last_part = res[-500:]
                            c_prompt = f"""
                            CONTEXT: ƒêang g·ª° bƒÉng d·ªü dang.
                            M·ªé NEO: "...{last_part}"
                            NHI·ªÜM V·ª§: T√¨m m·ªè neo, vi·∫øt ti·∫øp NGUY√äN VƒÇN ƒëo·∫°n sau. KH√îNG vi·∫øt l·∫°i m·ªè neo.
                            """
                            
                            safety_settings = [
                                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                            ]
                            
                            # L∆∞u tr·∫°ng th√°i cho Retry
                            st.session_state.last_prompt = c_prompt
                            st.session_state.last_config = cont_config

                            c_res = model.generate_content(
                                [c_prompt] + st.session_state.gemini_files, 
                                generation_config=cont_config,
                                safety_settings=safety_settings
                            )
                            
                            safe_c_text = get_safe_response(c_res)

                            if len(safe_c_text) < 50 or "k·∫øt th√∫c" in safe_c_text.lower() or "[D·ª™NG:" in safe_c_text:
                                st.session_state.is_auto_running = False
                                st.success("‚úÖ ƒê√£ xong!")
                                if "[D·ª™NG:" in safe_c_text:
                                    st.session_state.analysis_result += "\n\n" + safe_c_text
                                    st.rerun()
                            else:
                                st.session_state.analysis_result += "\n\n" + safe_c_text
                                st.session_state.loop_count += 1
                                st.rerun()
                        except Exception as e:
                            err_msg = str(e)
                            if "429" in err_msg or "Quota" in err_msg:
                                st.session_state.quota_error = True
                                st.session_state.is_auto_running = False # D·ª´ng auto ƒë·ªÉ x·ª≠ l√Ω l·ªói
                                st.rerun()
                            elif "404" in err_msg:
                                # 404 th√¨ t·ª± fallback lu√¥n
                                try:
                                    fb_model = genai.GenerativeModel("models/gemini-1.5-flash")
                                    c_res = fb_model.generate_content([c_prompt] + st.session_state.gemini_files, generation_config=cont_config, safety_settings=safety_settings)
                                    st.session_state.analysis_result += "\n\n" + get_safe_response(c_res)
                                    st.session_state.loop_count += 1
                                    st.rerun()
                                except: st.error("L·ªói h·ªá th·ªëng."); st.session_state.is_auto_running = False
                            else:
                                st.error(f"L·ªói: {e}")
                                st.session_state.is_auto_running = False

    with tab_chat:
        st.header("üí¨ Chat")
        if st.session_state.gemini_files:
            for m in st.session_state.chat_history:
                with st.chat_message(m["role"]): st.markdown(m["content"])
            if inp := st.chat_input("H·ªèi AI..."):
                st.session_state.chat_history.append({"role": "user", "content": inp})
                with st.chat_message("user"): st.markdown(inp)
                with st.chat_message("assistant"):
                    try:
                        m = genai.GenerativeModel(model_version)
                        r = m.generate_content(
                            st.session_state.gemini_files + [f"Tr·∫£ l·ªùi: {inp}"],
                            safety_settings=[{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}]
                        )
                        st.markdown(r.text); st.session_state.chat_history.append({"role": "assistant", "content": r.text})
                    except: st.error("L·ªói chat.")
        else: st.info("üëà Upload file tr∆∞·ªõc.")

if __name__ == "__main__":
    main()
